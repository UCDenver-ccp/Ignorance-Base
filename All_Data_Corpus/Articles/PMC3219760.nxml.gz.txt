Optimal management of iron deficiency anemia due to poor dietary intake

Abstract
Iron is necessary for the normal development of multiple vital processes. Iron deficiency (ID) may be caused by several diseases, even by physiological situations that increase requirements for this mineral. One of its possible causes is a poor dietary iron intake, which is infrequent in developed countries, but quite common in developing areas. In these countries, dietary ID is highly prevalent and comprises a real public health problem and a challenge for health authorities. ID, with or without anemia, can cause important symptoms that are not only physical, but can also include a decreased intellectual performance. All this, together with a high prevalence, can even have negative implications for a community’s economic and social development. Treatment consists of iron supplements. Prevention of ID obviously lies in increasing the dietary intake of iron, which can be difficult in developing countries. In these regions, foods with greater iron content are scarce, and attempts are made to compensate this by fortifying staple foods with iron. The effectiveness of this strategy is endorsed by multiple studies. On the other hand, in developed countries, ID with or without anemia is nearly always associated with diseases that trigger a negative balance between iron absorption and loss. Its management will be based on the treatment of underlying diseases, as well as on oral iron supplements, although these latter are limited by their tolerance and low potency, which on occasions may compel a change to intravenous administration. Iron deficiency has a series of peculiarities in pediatric patients, in the elderly, in pregnant women, and in patients with dietary restrictions, such as celiac disease.

Introduction
Iron is highly important for human beings, since it is involved in multiple processes of cell energy metabolism, in which its presence is essential. However, an excess of this metal is very harmful. Both reasons explain its very finely regulated metabolism, as it achieves a rather narrow balance. Perhaps because an organism does not have a physiological means for eliminating the excess of remaining iron, its intestinal absorption is low, similar to its physiological losses, and iron can increase only slightly, even in cases of deficiency. This accounts for the frequent development of iron deficiency (ID) when there are concomitant factors that increase its losses or requirements, or when its intake decreases. Regardless of its etiology, the World Health Organization (WHO) estimates that anemia affects over 1.62 billion people worldwide. The most affected group is preschool-age children, with a prevalence of 47%, followed by pregnant women (41%), non-pregnant women (30%), school-age children (25%), and people older than 60 years of age (24%); men are the least affected group (12%). However, globally, the most numerous population group is non-pregnant women (468.4 million). In terms of geographical distribution, the most affected regions in all studied groups are Africa and South East Asia, while the least affected areas are Europe and the Western Pacific.1According to WHO data, the frequency of ID in developing countries is 2–5 times greater than that of iron deficiency anemia (IDA), due to various factors, such as infections and malnutrition. In light of all this information, the importance of ID worldwide in the twenty-first century is evident.
Next, we will review some of the most important aspects of ID, or IDA, focusing primarily on their treatment, and, more specifically, when the etiology is an inadequate iron intake.

Metabolism of iron, etiopathogenesis, and development of ID and IDA
Iron is the most abundant trace element in humans. Its main role, but not the only one, is to form part of hemoglobin, an essential protein for oxygen transport. Moreover, it mediates other chemical reactions that are critical for life, forming part of the enzymes implicated in DNA synthesis and cell respiration. These physiological roles are carried out due to its ability to uptake and donate electrons, interchanging between its ferric (Fe3+) and ferrous (Fe2+) forms. For similar reasons, it can also be very toxic for tissues, as it intervenes in the formation of free radicals. Therefore, its metabolism is very finely regulated and practically all the iron in our body is bound to proteins. The average daily dietary iron intake in developed countries is between 10 and 15 mg;2under normal conditions, only between 5% and 10% of this quantity is absorbed, that is, 1–3 mg/day. This absorption takes place primarily in the duodenum and proximal jejunum, and it is complex and highly regulated, as the key point in iron metabolism. Dietary iron can be found in two forms: heme iron and non-heme iron. Heme iron is contained in hemoglobin or myoglobin, and is derived almost exclusively from meat (veal, pork, lamb, chicken, turkey, and others), fish, and seafood. Non-heme iron (especially ferric salts) is most frequently found in vegetables (legumes, corn, wheat, barley, and others), but also in animal-source foods, iron-fortified foods such as rice, pasta, bread, and other cereals, cooked spinach, nuts, seeds and dry fruits, eggs, and dairy products. The absorption of iron in these two forms is different. Heme iron has a greater bioavailability, with absorption of 20%–30%. Thus, once the heme group of proteins containing it is released, it remains soluble and is taken up easily, although by a mechanism not entirely known. Non-heme iron, the most abundant in the diet, can be found either reduced or oxidized. Facilitated by gastric acidity, iron in oxidized form is reduced (Fe++) by an enzyme present in the enterocyte brush border, being absorbed at a rate of less than 10%. This absorption is maximized by ascorbic acid and foods with a substantial content of heme iron, but is inhibited by calcium, phytates, and polyphenols.Table 1shows the amounts of iron in different foods. Enterocytes are the key element in this absorption process. Iron, in its reduced form, enters the enterocyte cytoplasm by means of a membrane transporter. At least three main mechanisms have been identified in the regulation of this absorption process. The most important is erythropoietic requirements, in such a way that absorption increases with increasing requirements. Thus, in anemic individuals, absorption increases, although up to no more than 20–40 mg/day.3The second mechanism is dietary iron content, so that absorption decreases as dietary content increases. The third main mechanism is a reduction in the iron pool, which increases its absorption. Once absorbed and inside the enterocyte, iron moves on to the plasma and, transported by transferrin, reaches the cells where it will be used, especially in erythropoietic precursor cells in bone marrow. The uptake of iron-transferrin complexes in all these cells is a process that is also mediated by receptors, resulting in the intra-cytoplasmic release of iron. Once in cytoplasm, in hematopoietic cells, iron enters the mitochondria to participate in the synthesis of the heme group, an essential component of hemoglobin. In the remaining cells, it can mediate other metabolic processes (eg, synthesis of myoglobin or some enzymes such as catalases and peroxidases) or be stored in the form of ferritin or hemosiderin, to be used when necessary. Ferritin, present especially in the liver, stores iron in a soluble, mobilizable way, whereas hemosiderin, mainly located in the smooth endoplasmic reticulum, stores non-mobilizable iron. Another key molecule in iron metabolism is hepcidin, a hepatocyte-produced peptide hormone that modulates the passing of iron from cells (enterocytes, macrophages, and hepatocytes) to the plasma.4
From a pathogenetic standpoint, we can classify the causes of ID/IDA into two groups: those involving a reduced intake, and those involving increased losses. Furthermore, some physiological situations can increase the body’s iron requirements, promoting its deficiency, for example, periods of rapid growth, pregnancy, and reproductive age in women (menstrual losses).Table 2summarizes the recommended daily amounts of iron by age and gender.5The relative frequency of different etiologies of ID and IDA depends on these factors (seeTable 3). However, globally, the most influential aspect is the degree of economic development of the analyzed population. In developed countries, ID and IDA will be a consequence mainly of conditions that increase iron losses (any medical condition that involves bleeding) or, to a lesser extent, of disorders that cause a decreased absorption (eg, celiac disease) or perhaps even gastritis due to Helicobacter pylori , the likely origin of ID/IDA refractory to oral iron supplementation, which usually reverts once the germ has been eradicated, even without adding oral iron,6or both processes jointly. In our setting, insufficient dietary iron intake, a consequence either of overall malnutrition or specific dietary habits (eg, in strict vegetarians), is usually not a cause of ID or IDA. Childhood and adolescence, women in childbearing age, and pregnancy are situations in which the prevalence of ID and IDA is much higher and is not always accompanied by pathologies. In the elderly, some physiological changes are present that also make the development of ID and IDA more likely; nonetheless, these changes do not justify their development and the investigation of other very relevant etiologies is required. In developing countries, the situation is very different. In this case, malnutrition and a poor dietary iron intake are very common and are frequently the cause of ID or IDA. Coexistence of other factors is common in the genesis of ID, such as intestinal parasites.

Consequences of iron deficiency in humans
Iron deficiency has a great impact on the organism involved, as it interferes in all the reactions in which iron is involved. This will eventually result in well-known clinical consequences. The clinical presentation varies greatly from one case to the next, depending on the rapidity of onset of anemia, its severity, and the characteristics of the patient suffering from it. The cause of ID/IDA is another influencing factor, as well as the existence of other added nutritional deficiencies, or even a protein-calorie malnutrition. Thus, IDA or ID can be detected in subjects that are asymptomatic or when they present with systemic symptoms, more or less important, such as weakness, tiredness, irritability, lack of concentration, headache, intolerance to exercise, or even a clinical presentation of heart failure in more serious cases. Some patients with ID, with or without anemia, can experience alopecia, atrophy of lingual papillae, or dry mouth due to decreased salivation. Other types of symptomatology, such as frailty of nails or spoon nails (koilonychia), chlorosis, or Plummer- Vinson or Paterson-Kelly syndromes (with dysphagia due to esophageal membranes and atrophic glossitis) have practically disappeared. The most dramatic consequences of ID are increases in both maternal and prenatal/perinatal mortality, apart from an increase in prematurity. In fact, 40% of perinatal maternal deaths are associated with anemia.7Iron deficiency has also been shown to impair cognitive performance and physical abilities, thus affecting performance at school and work.8,9The immune system is also affected by several mechanisms that influence the correct functioning of leukocytes,6thus increasing morbidity from infectious diseases. Thyroid function can be affected by various mechanisms that finally result in a reduction in triiodothyronine and an increase in thyroid peroxidase activity.10
It is important to point out that not only IDA, but also “simple” ID can cause symptoms, as has been shown in several studies. Furthermore, it is necessary to outline that some of these consequences are hardly quantifiable. Hence, ID has a clear negative impact on the quality of life of people who suffer it, in spite of not being easy to estimate this correctly. It is even more complicated to analyze other effects it most surely produces. We are referring here to its impact on school and work performance as well as its economic implications for patients and society. It is highly possible that the development of human beings under conditions of ID results in a lower intellectual and educational development that, in the long term, will adversely influence their professional future and derived income. It could even be one of the reasons for slow development in poorer countries, hindering the adequate development of its human capital. Moreover, let us not forget that, in the event of ID in diet, there will most certainly be other deficiencies that will also have a negative impact on the adequate development of individuals. This is especially important in certain restrictive diets (either medically or not medically indicated), since some diets have an evident indication for health purposes, such as gluten-free diets for celiac people, allergy diets, ketogenic diets in the management of metabolic disorders, and nutritional support diets in specific conditions affecting the gastrointestinal tract (eg, postoperative patients); thus, it is not rare to identify calcium, zinc, tiamine, riboflavine, niacin, and folate deficiencies in these kinds of diets/patients.11

Diagnosis of ID and IDA
ID is defined as a decrease in total iron levels (both in serum and deposits), regardless of clinical expression. When the deficiency impairs erythropoiesis, it may result in a decrease in hemoglobin, with subsequent development of IDA. The diagnosis of anemia is simple and objective: the WHO defines it as a decline in blood hemoglobin to a level less than 13 g/dL in men and 12 g/dL in women. However, confirming that ID is the mechanism responsible for this anemia is not always easy. Sometimes, a simple blood cell count strongly suggests this origin, with a typical pattern of microcytosis, hypochromia, and an elevated red cell distribution width (RDW). However, up to 40% of cases of “pure” IDA are normocytic. The next step in assessing anemia is to determine the so-called iron metabolism. A typical pattern of ID is a decrease in sideremia, plasma ferritin, and transferrin saturation. The least reliable parameter for diagnosis of ID is probably the measurement of sideremia (total amount of iron bound to serum proteins), as it could be detected as an artifact due to contamination of laboratory equipment, shows a nocturnal rhythm, and can return to normal levels several hours after ingestion. In the absence of inflammation, serum ferritin reflects total iron deposits in the body. Thus, a low serum ferritin (<30 ng/L) unequivocally means ID. Transferrin saturation is especially useful when the measurement of ferritin is equivocal; a percentage of transferrin less than 20% implies ID.12Zinc protoporphyrin is another indicator of IDA and early iron depletion. When iron supply is diminished, zinc utilization increases resulting in a high zinc protoporphyrin.13Once it has been established that ID is the cause of anemia, the next step is to diagnose why ID is present. Differential diagnosis will take into account the most frequent etiologies in each clinical scenario.

General management of ID and IDA anemia due to poor dietary intake
In a simplistic manner, when ID or IDA due to poor iron intake is present, the mainstay of its treatment and prevention is to normalize the amount of dietary iron. The problem is that this is not so easy in most patients under these circumstances, as it involves an economic effort they may not undertake. Having said this, we will take a graphical overview of what should be the general management in these cases, and later we will comment on the particularities of some specific situations. Of course, if another etiology associated with an inadequate dietary iron intake is evidenced when investigating the cause of anemia, it should be treated specifically.

Iron supplementation
The first measure in initial treatment is to provide iron supplements until the values of hemoglobin and iron deposits have been normalized. This will usually be achieved by administering iron orally.14Even though it is broadly recommended to take it before breakfast in order to increase its absorption, there is no evidence supporting this, and it does significantly reduce tolerance; thus, it seems reasonable to administer it with foods (Table 4). One tablet of any commercially available preparation of ferrous salts provides more iron than intestines are able to absorb in 1 day. All ferrous salts (fumarate, lactate, succinate, glutamate, or sulfate) have similar bioavailability. Iron glycinate preparations are a therapeutic alternative due to excellent bioavailability and lower frequency of side effects, such as constipation.15,16Other formulations (several complexes of iron and proteins) may serve as substitutes for ferrous salts, but there is no strong evidence (from controlled clinical trials) showing these preparations are superior in terms of tolerance or clinical response. The treatment with oral iron is slow in reaching its goals, and a good compliance is necessary to be successful. The therapeutic goals are recovery from anemia and normalization of iron deposits. In any case, oral iron has important limitations. Often, the anemia is severe, and a quick response is needed. In other occasions, tolerance is poor, even following the precautions we have recommended, and this is especially frequent in chronic bowel inflammatory disease. Finally, in other patients, it may not be intrinsically effective due to a problem of inadequate absorption, or because losses are greater than the limited capacity of oral assimilation. In these situations (poor tolerance, inadequate compliance, ineffectiveness, or clinical emergency), the use of parenteral iron is fully justified.14The efficacy and safety of iron sucrose have been demonstrated in clinical trials and have been confirmed by broad practical experience.17The only inconvenience with iron sucrose is that the infusion of several doses is necessary in order to provide the total required dose. More recently, new formulations such as carboxymaltose iron18have appeared, making it possible to provide the entire required dose in just one or two sessions. In order to calculate the amount of iron a patient needs, a formula is used: Total iron deficiency (mg) = [weight (kg) × (target Hb − specific patient’s Hb (g/L) × 0.24) + 500 (approximate deposits), where Hb is hemoglobin and 0.24 is a constant. Side effects are very uncommon and nearly always mild, except for extremely uncommon anaphylactoid (pseudoallergic) reactions; nonetheless, observation and measuring of vital signs during the administration of the infusion are required.

Dietary improvement
Secondly, in order to prevent the new development of ID or IDA, it is necessary to correct the patient’s diet. This is not always easy, especially in developing countries, due to economic determinants. The daily requirements of iron are 1–3 mg/day; these requirements increase during the growth period, in women of childbearing age, and in pregnant women, and decrease due to the cessation of menses. Because gastrointestinal absorption of iron is limited, the diet must contain between 15 and 30 mg/day. Food-frequency questionnaires combined with information on meal composition and food consumption patterns are necessary to analyze the amount of iron ingested by a person.7Several kinds of questionnaires in multiple clinical scenarios have been used to assess the quantity of daily iron intake; readers are encouraged to consult some especially interesting articles relating to such questionnaires.19–21
The primary goal of dietary modification, that is, improving and maintaining the iron status of a population, involves changes in behavior, leading to an increase in the selection of iron-containing foods and a meal pattern that favors increased bioavailability. Efforts should be focused on promoting the access to iron-rich foods (eg, meat and organs from cattle, fowl, fish, and poultry, and non-animal foods such as legumes and green leafy vegetables) and foods that enhance iron absorption (some fruits, vegetables, and tubers). Recommendations should be adapted to regional and local variations in diet and the age groups concerned. In the specific case of populations following diets based on vegetable consumption with scarce meat intake, frequent in developing countries (and also in vegetarians), in which iron is non-heme, and hence, ID is more likely,22absorption may be increased by promoting the intake of foods that favor this, such as vitamin C,23,24and avoiding those that hinder it, such as calcium, very fatty foods, phytates, and others.25Table 5shows some of these considerations in greater detail.

Iron fortification
Another approach to improve ID in large populations is food fortification. Iron fortification is a practical and cost-effective long-term solution to control ID on a nationwide scale. A program of effective iron fortification mandates cooperation efforts among governments, food industry, and consumers. Iron fortification of foods is more difficult than fortification with other nutrients, such as iodine in salt. Iron compounds that are more soluble and absorbable, such as ferrous sulfate, often react with other ingredients in food, oxidize fats, and produce an unpleasant taste and color changes. Hence, other iron compounds are used, such as ferric pyrophosphate, or electrolytic iron, and the amount required is twice that of ferrous sulfate, as they are less absorbable in the gastrointestinal tract, but they produce no sensory changes in foods.22,26,27In any case, the appearance of these sensory changes should be monitored during the processing, storage, and preparation of foods. Moreover, it is necessary to identify an appropriate alimentary vehicle that reaches the entire target population, such as rice, pasta, or bread. Foods that are used most frequently in the fortification of populations are flours of staple cereals.
Several fortifiers have been used successfully in nationwide programs in different countries, and no significant side effects have been described with this fortification. In a meta-analysis of studies carried out in children receiving iron-fortified foods, no adverse effects occurred and a protective effect against the development of respiratory infections was observed with fortification.27It becomes necessary to use specific laboratory parameters relating to iron levels (such as ferritin or erythrocytic protoporphyrin) to evaluate the fortification needs in a population and to monitor subsequently the results of such intervention.

ID and IDA in specific populations

ID in pediatric population
Iron requirements are particularly high during periods of rapid growth such as childhood,5resulting in stages of high risk for ID. The first period ranges from 6 to 12 months of age, a period characterized by a certain imbalance between provisions (continuation formulas with different proportions of iron fortification, introduction of different foods at unequal rhythms and amounts) and requirements (Table 2). The second period includes the entire duration of breastfeeding (1–3 years).
The high prevalence of IDA in childhood at the beginning of the twentieth century in the United States made this condition acquire the relevance of a public health problem. The strategy of universal screening, together with the promotion of breastfeeding, the generalization of iron supplementation in artificial formulas and cereals, as well as the creation of a specific program in 1972 (WIC. “Special Supplemental Food Program for Women, Infants, and Children”) that included education, oriented both to professionals and the general population, had a very positive impact; thus, in the 1980s, the prevalence of IDA was reduced dramatically in the entire socioeconomic spectrum of the United States.28This success turned screening into an approach restricted only to high-risk populations.
On the other hand, IDA is associated with the development of mental, motor, and behavioral changes in pediatric patients. The brain has a homeostasis mechanism to protect itself from both iron depletion and overload.29Iron is heterogeneously distributed in the brain, with higher levels in white matter than in grey matter. It appears that neurons and glia cells accumulate iron via a transferrin-dependent mechanism. Inside cells, it is bound to proteins as an accumulation reserve (ferritin) or functional reserve (bound to enzymes). This dual pathway serves as a protection element against abrupt changes in iron serum levels within certain ranges.30A recent review has documented such alterations: a tendency to fail in school, worse mathematical calculation, worse results in comprehension and development of reading-writing skills, as well as lower scores in complex psychomotor tests.31Moreover, there is a greater tendency to present with conflictive behavior with parents and teachers. Many of these deficiencies or alterations can be corrected with iron administration. However, using finer mental psychometric and psychomotor tests, subclinical alterations have been detected that are not identifiable in routine physical examinations, even after adequately correcting iron deposits. Some authors have outlined the importance of iron in the brain’s normal functioning, having described the existence of a correlation between iron levels and cognitive capacity.32
Two great population studies, The Health and Nutrition Examination Survey (NHANES III) and the Third Report on Nutrition Monitoring in the United States (1988–1991),32evidenced a prevalence of IDA of 3% to 15% in children between 1 and 3 years of age. Several causes are responsible: on the one hand, the low socioeconomic level that determines inadequate diets in many cases; on the other hand, in this age group, children are incorporated into a “normalized diet together with adults” in which iron-supplemented foods stop being administered, and on occasions, deficient diets are implemented.

Prevention and treatment of ID in children
For primary prevention, almost all clinical guidelines recommend supplementation in asymptomatic infants between 6 and 12 months of age at risk for ID.33
Iron-fortified diets improve hematologic parameters in populations, as evidenced in a previously mentioned historical series assessed in the United States following the implementation of Special Supplemental Food Program for Women, Infants, and Children (WIC) in 1972, which dramatically decreased the prevalence of IDA in the entire socioeconomic range of the United States.34Iron-fortified foods can be useful to provide daily doses of this nutrient to populations, especially in developing countries.35However, in developed areas, people on iron-poor diets or at risk for ID need to be identified and counseled individually.27
Infants considered at risk include those of low socioeconomic status, Africans or African Americans, immigrants from developing countries during the first 2 years of residency, those born preterm and small for gestational age, as well as those who did not receive an adequately supplemented continuation formula. Likewise, health education is recommended for the promotion of breastfeeding, as well as to promote iron-fortified diets in infants and pregnant women.
For secondary prevention, the US Preventive Task Force recommends screening pregnant women for ID,36but there is not enough evidence to make recommendations in favor or against routine screening in asymptomatic people at any other age.37Infants with one or more risk factors should be subject to screening for ID. The introduction of cow’s milk in the first year of life and the socioeconomic level are factors to take into consideration in the investigation.
Screening during the first year of life is carried out between 9 and 12 months of age. Especial consideration must be given to preterm infants and those small for gestational age, whose screening must be conducted at 6 months and every 3 months in those not receiving iron supplements or iron-supplemented formulas. After 12 months, any at- risk infant should be studied at the slightest doubt about symptoms. A positive result in screening tests requires confirmation with an iron-treatment trial. The ideal screening tool should allow ID to be discovered before the development of subsequent anemia or associated deficits. The usual screening test has been hemoglobin or hematocrit levels; however, these parameters are altered only when ID is very severe. Ferritin levels, transferrin saturation index, and erythrocytic protoporphyrin levels have greater sensitivity and specificity, but because their cost is higher, they are not to be used routinely. Out of all these diagnostic tests, erythrocytic protoporphyrin levels are more cost-effective, and some clinical studies have shown their effectiveness as screening tools in field settings.38,39The validation of hematofluorometry methods for estimating erythrocytic protoporphyrin combined with hemoglobin levels shows a promising future in screening strategies. Obtaining the RDW, combined with hemoglobin levels, may be an option in cases where protoporphyrin cannot be measured. The elevation of RDW is an early indicator of ID, and almost all blood analyses provide it automatically along with hemoglobin. Other markers, such as transferrin soluble receptor or content of erythrocytic hemoglobin, are currently being evaluated. In areas where it is not possible to carry out a complete blood count or an evaluation of erythrocytic protoporphyrin, screening with hemoglobin must not be abandoned. Moreover, hemoglobin continues to be the parameter that determines the cut-off point for different ages, genders, and even races, in terms of definition of anemia.
After detecting IDA, it is recommended to carry out an iron stores filling test, which implies treatment with iron for 1 month, repeating the hemoglobin test, and if an increase of 1 g/dL or greater is observed in the values of hemoglobin (or three points in the hematocrit), the existence of ID can be concluded (even if the baseline hemoglobin is practically normal). This treatment with iron supplements is administered in children at a dose of 3–6 mg/kg/day of elemental iron, distributed in several doses. If ID prevails after the adequate treatment with oral iron, a complete etiological study in all cases is indicated.

ID in pregnant women
During pregnancy, women need iron and folic acid for their own metabolism and that of the fetus. Anemia, ID, and folate deficiency increase the risk of neural tube defects, preterm births, and low birth weight. Adequate iron levels in the mother are essential for preventing ID in the fetus. Several studies have shown that joint supplementation of iron and folic acid during pregnancy significantly reduces preterm births and early neonatal mortality, one of the main causes of infant mortality in developing countries, compared with the administration of folic acid alone.40–42A comprehensive review of the use of iron and folic acid supplements in pregnancy suggests that the reduction of anemia is similar following daily or weekly dosing regimens, which means that iron dosing regimens in pregnancy should be revised in order to continue achieving the desired effect while minimizing side effects.43Women with established IDA should receive iron supplements additionally to the usual recommended dose (see section on treatment). Intolerance to the different oral iron preparations is not uncommon, especially during gestation, a period in which the gag reflex is increased. In these cases, the need for an intravenous treatment must be individualized for each patient (likely starting from the second or third trimester of pregnancy).

IDA in the elderly
Anemia is very frequent among the elderly; hence, numerous studies have been conducted in order to attempt to define if there are different ranges of normality in different age groups that would enable selection of those patients who would require a clinical assessment.44Thus, in those over 85 years of age, using the WHO definition of anemia, up to 26% of men and 20% of women would be suitable for an etiological study.45Anemia in the elderly is clinically very relevant and several studies have linked it to an increase in overall morbidity, a higher rate of Alzheimer’s disease, worse results in quality of life scores, a greater number of hospital admissions, and a higher overall mortality.46,47
On the importance of nutritional deficiency as a cause of anemia in the elderly, the study NHANES III suggested its involvement in one-third of the patients over 65 years of age.48Most of these cases were due to ID, including chronic losses, although a percentage was associated with folate or vitamin B12 deficiencies. Other significant etiologies of anemia in the elderly that should be taken into consideration in the differential diagnosis of IDA are anemia due to chronic disorders (including the very common presence of chronic renal disease) and myelodysplastic syndromes.
In conclusion, the high prevalence of anemia in elderly patients (higher in men than in women) can be influenced by using cut-off points that are too high when it comes to defining anemia. However, anemia is more frequent in patients over 65 years of age, and much more in those over 85 years of age (“the oldest old”) due to all factors presented above. In any case, it is very important to keep in mind that other very important conditions, such as tumors, are usually involved in the development of IDA in the elderly. The diagnostic and therapeutic approach must be as usual; nonetheless, the possibility of dietary deficiency as a cause or promoting factor should be considered on an individual basis.

Deficiencies in patients with dietary restrictions: patients with celiac disease
Numerous patients, for several reasons, follow a restricted diet more or less strictly: diets for allergic patients, ketogenic diets, diets through probes, tubes or ostomies, vegetarian diets, and macrobiotic diets, for example. We will focus on celiac disease. In untreated patients, the absorption of iron and other nutrients is frankly compromised48,49and usually causes multiple deficiencies, such as iron, folates, and calcium. Malabsorption of fat-soluble vitamins (ADEK), carbohydrates, fats, and other micronutrients can occur if more distal segments of small bowel are affected. Deficits in vitamins B12 and B6, as well as zinc, selenium, and copper, have also been described. Once the diet has been established, absorption normalizes. However, some studies suggest that even celiac patients who are “strict compliers” with a gluten-free diet can have dietary deficiencies, derived from the imbalance between proteins, carbohydrates, and fats, which, in turn, result in deficits of different micronutrients.48,49At the point of diagnosis, it is usually advisable, apart from an adequate gluten-free diet and correcting the confirmed deficiencies, to prescribe gluten-free vitamin supplements. Changing and combining cereals and flours may provide more quality to the diet. Thus, as an alternative to wheat, amaranth, buckwheat, millet, whole rice or brown rice, quinoa, soy, or Indian rice can be used. Another substitute for wheat available is millet flour. When it is uncertain whether the patient’s diet is sufficiently balanced, it is reasonable to complement it with the referred gluten-free multivitamin complexes.50

IDA in developing countries
It is in this scenario where anemia due to poor iron dietary intake is most relevant. In many of these countries, the prevalence of malnutrition is considerable, and iron intake is often inadequate. The WHO has estimated that, in developing countries, IDA is the third cause of disability-adjusted life years lost in 15- to 44-year-old women. For men in this age group, anemia consistently ranks in the top 10 causes of morbidity.38Infants and women of childbearing age are mostly affected. Malnutrition, with the consequential ID, is found in up to 53% of conditions that cause the greatest infant morbidity and mortality. The purely nutritional problem is aggravated in these countries because they usually do not implement programs for iron fortification of feeding formulas. As we indicated previously, several studies have shown that iron has a direct influence on birth weight, as well as on the incidence of preterm births, although the designs of these studies have been criticized and the implications for clinical daily practice are limited.51
We must remember that although poor iron intake is the main cause of anemia in developing countries, numerous factors may be associated and represent the etiology or promoting factors of anemia in these populations (Table 6). Hence, the control of parasitic infections in developing countries is of considerable importance in public health. Mild infections with multiple parasitic species have been associated with an increased frequency of anemia in most developing countries.52Hookworm and Schistosoma spp. infections are leading causes of anemia in these countries and co-administration of drugs with multiple courses of treatment has been an effective strategy.53Nonetheless, the problem cannot be overcome by preventive chemotherapy alone. In order to achieve a sustainable control of parasitic infections and prevention of anemia, access to safe and effective drugs, complemented with health education, improvements in water supply, and adequate sanitation are necessary.54

PubMed Central:	